<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-03-07T15:56:47-03:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">GFickel Blog</title><subtitle>A Blog mostly geared toward Machine Learning and high performance code.</subtitle><entry><title type="html">Making your GPU go BRRR: Creating a CUDA Layer in PyTorch</title><link href="http://localhost:4000/jekyll/update/2024/02/23/making-your-gpu-go-brrr-programing-a-network-layer-in-cuda.html" rel="alternate" type="text/html" title="Making your GPU go BRRR: Creating a CUDA Layer in PyTorch" /><published>2024-02-23T12:00:00-03:00</published><updated>2024-02-23T12:00:00-03:00</updated><id>http://localhost:4000/jekyll/update/2024/02/23/making-your-gpu-go-brrr-programing-a-network-layer-in-cuda</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/02/23/making-your-gpu-go-brrr-programing-a-network-layer-in-cuda.html"><![CDATA[<p>I still remember the “dark ages” of research, when I was still doing my masters when it was common to find really impactful publications that provided no code. And yes, I’ve sent my fair share of emails to authors… Fortunately, this is no longer the norm, and even somewhat frowned upon. Caffe, Tensorflow, Keras, PyTorch, and even more deep learning frameworks really helped everyone to create way smaller, cleaner code, that was also easier to share.</p>

<p>Those frameworks are really incredible and allow us to quickly implement and test new ideas, however, they are not always the fastest way, even if they use CUDA down the line. This is definitely becoming a bottleneck. PyTorch 2 implemented a compile process to fuse layers to improve GPU usage, Flash Attention did the same by directly programming Attention in CUDA and achieved an even greater runtime improvement. And recently I’ve seen a trend to reimplement slower Python code in Rust <a href="https://github.com/huggingface/tokenizers">like this</a> one from Hugging Face.</p>

<p>CUDA programming may seem intimidating, at least it was for me. I first learned circa 2010 and it was a really bad development experience, but by watching an <a href="https://www.youtube.com/watch?v=nOxKexn3iBo">awesome video</a> by Jeremy Howard, I’ve learned that it is indeed possible to have a much better experience. The main idea is the following:</p>

<ol>
  <li>Implement the forward and backward pass in PyTorch. This gives access to an online debugger and the full functionality of Python, like Jupyter Notebooks.</li>
  <li>Validate the implementation with gradcheck. This somewhat magic function runs your forward pass and does numerical derivation to validate your backward pass code.</li>
  <li>Program the CUDA Kernel for forward and backward passes using Numba, directly in Python. This is the real thing, where we are dealing with CUDA threads and possibly memory management.</li>
  <li>Ask <a href="https://chat.openai.com/">Chat-GPT</a> to convert this code to C CUDA. Really, it works surprisingly well!</li>
  <li>Use PyTorch internal functionality to compile this C CUDA to a Python module that you can use with torch tensors.</li>
  <li>Use gradcheck again to verify that your CUDA written layer is 100% correct.</li>
</ol>

<p>It may be a couple of hoops, but the ability to develop CUDA code in Python makes our lives so much easier. You have easier integration with debugers, and the iteration time between changes in code and running it is nearly instant, compared to the long time it takes to compile C CUDA. You may noticed that mentioned both forward and backward passes, and unfortunately, if we use CUDA for our backward pass, we can’t rely on autograd to get this for us. But fortunately, we have this amazing function from PyTorch, <a href="https://pytorch.org/docs/stable/notes/gradcheck.html">gradcheck</a>, that will validate for us if our backpropagation is indeed correct.</p>

<p>We need some kind of end goal, and for us, it will be the implementation of the Sigmoid activation inspired by <a href="https://www.youtube.com/watch?v=oxC3T_-_Amw">David Oniani</a>. You’ll see that it has some interesting characteristics that will help us explore interesting (and important) aspects of creating a performant CUDA layer.</p>

<h2 id="1-forward-and-backward-passes-in-pytorch">1. Forward and Backward passes in PyTorch</h2>

<p>The idea here is to do two functions: one for the forward pass and the backward one. But first, let’s remember the formula for the sigmoid and its derivative:</p>

\[\sigma(x) = \frac{1}{1+e^{-x}}\]

\[\sigma^{'}(x) = \sigma(x)(1-\sigma(x))\]

<p>Those are not that complicated to implement, especially the derivative that only depends on the value of the sigmoid that we already computed on the forward pass. However, this sigmoid equation does present some numerical instabilities, so it is better to implement the following:</p>

\[\sigma(x)=\begin{cases}
\frac{1}{1+e^{-x}} &amp; \text{ if } x&gt;=0 \\ 
\frac{e^{x}}{1+e^{x}} &amp; \text{ if } x&lt;0 
\end{cases}\]

<p>With this in mind, we can generate the following Python code:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">sigmoid_forward_torch</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="n">out_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">empty_like</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">positive_mask</span> <span class="o">=</span> <span class="nb">input</span> <span class="o">&gt;=</span> <span class="mi">0</span>
    <span class="n">out_tensor</span><span class="p">[</span><span class="n">positive_mask</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="nb">input</span><span class="p">[</span><span class="n">positive_mask</span><span class="p">]))</span>
    <span class="n">out_tensor</span><span class="p">[</span><span class="o">~</span><span class="n">positive_mask</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="nb">input</span><span class="p">[</span><span class="o">~</span><span class="n">positive_mask</span><span class="p">])</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="nb">input</span><span class="p">[</span><span class="o">~</span><span class="n">positive_mask</span><span class="p">]))</span>
    
    <span class="k">return</span> <span class="n">out_tensor</span>

<span class="k">def</span> <span class="nf">sigmoid_backward_torch</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">input</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="nb">input</span><span class="p">)</span>
</code></pre></div></div>

<p>Notice that I’ve used a variable called <em>positive_mask</em> to create an index to identify positive and negative input values. Other than that, the code is somewhat straightforward.</p>

<h2 id="2-check-our-derivatives">2. Check our Derivatives</h2>

<p>Now that we have a Python code to do our forward and backward pass we can test if they are coherent with each other. In other words, we will use <a href="https://pytorch.org/docs/stable/notes/gradcheck.html">gradcheck</a> from PyTorch to run a forward pass, compute numerically what the derivative should be, and check our backward pass result. But first, we must set it within its autograd format. It is not that complicated, and stays like this:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Sigmoid</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="n">Function</span><span class="p">):</span>
    <span class="s">"""The Sigmoid activation function."""</span>

    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="s">"""Performs a forward pass."""</span>

        <span class="n">out_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">empty_like</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">positive_mask</span> <span class="o">=</span> <span class="nb">input</span> <span class="o">&gt;=</span> <span class="mi">0</span>
        <span class="n">out_tensor</span><span class="p">[</span><span class="n">positive_mask</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="nb">input</span><span class="p">[</span><span class="n">positive_mask</span><span class="p">]))</span>
        <span class="n">out_tensor</span><span class="p">[</span><span class="o">~</span><span class="n">positive_mask</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="nb">input</span><span class="p">[</span><span class="o">~</span><span class="n">positive_mask</span><span class="p">])</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="nb">input</span><span class="p">[</span><span class="o">~</span><span class="n">positive_mask</span><span class="p">]))</span>
        
        <span class="n">ctx</span><span class="p">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">out_tensor</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">out_tensor</span>

    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="s">"""Performs a backpropagation."""</span>

        <span class="p">(</span><span class="n">result</span><span class="p">,)</span> <span class="o">=</span> <span class="n">ctx</span><span class="p">.</span><span class="n">saved_tensors</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">result</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">result</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">grad_output</span> <span class="o">*</span> <span class="n">grad</span>
</code></pre></div></div>

<p>Notice that both on forward and backward pass we are dealing with an additional variable: ctx. This is our context, that we can use to save some data on our forward pass to use on backward. This is quite handy for our Sigmoid since the backward pass is a simple formula that uses the forward pass result, so we save it on the context for our backpropagation.</p>

<p>Finally, on the backward pass, we get the sigmoid result that we stored on ctx and use it to compute its derivative. But we have another input, that is the input derivative that is being propagated to our layer. So our final gradient is this derivative multiplied by our sigmoid derivative.</p>

<p>With this in hand, we can call the following function to check if everything is correct:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sigmoid</span> <span class="o">=</span> <span class="n">Sigmoid</span><span class="p">.</span><span class="nb">apply</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">double</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="n">gradcheck</span><span class="p">(</span><span class="n">sigmoid</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'gradcheck successful :D'</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'gradcheck unsuccessful :D'</span><span class="p">)</span>
</code></pre></div></div>

<p>If everything is correct we are ready to think about how to implement it in CUDA, otherwise, we can back up and check what we did wrong.</p>

<h2 id="3-cuda-implementation-using-numba">3. CUDA Implementation using Numba</h2>

<p>(I will not dive into all the details on how CUDA works, but I suggest you check <a href="https://www.youtube.com/watch?v=nOxKexn3iBo">this video</a> by Jeremy Howard to see a great explanation about it!)</p>

<p>The first thing we need to do is decide how we are going to model this in CUDA. I believe the most sensible approach is to use a single thread for each element on the input Tensor, for both forward and backward passes. And to finally implement it, we can use the <a href="https://numba.pydata.org/">Numba library</a>, which is a JIT compiler for Python with support for CUDA, SIMD, and even threading. But for our case, we are more interested in the CUDA dev environment, especially the <a href="https://numba.pydata.org/numba-doc/latest/cuda/simulator.html#simulator">CUDA simulator</a>.</p>

<p>To start, the first thing we must do is set NUMBA_ENABLE_CUDASIM=’1’ as an environment variable before we import Numba. Then we just need to add the @cuda.jit decorator on top of our CUDA kernel function and we are good to go!</p>

<p>Let’s start with the following code for both the forward and backward passes:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">numba</span> <span class="kn">import</span> <span class="n">cuda</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="o">@</span><span class="n">cuda</span><span class="p">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">sigmoid_forward</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">input_len</span><span class="p">,</span> <span class="n">out</span><span class="p">):</span>
    <span class="n">cbi</span><span class="p">,</span><span class="n">cbd</span><span class="p">,</span><span class="n">tid</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="n">blockIdx</span><span class="p">,</span><span class="n">cuda</span><span class="p">.</span><span class="n">blockDim</span><span class="p">,</span><span class="n">cuda</span><span class="p">.</span><span class="n">threadIdx</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">cbi</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">cbd</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">tid</span><span class="p">.</span><span class="n">x</span>

    <span class="k">if</span> <span class="n">idx</span> <span class="o">&gt;=</span> <span class="n">input_len</span><span class="p">:</span>
        <span class="k">return</span>
    
    <span class="k">if</span> <span class="nb">input</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">res</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">/</span> <span class="p">(</span> <span class="mf">1.</span> <span class="o">+</span> <span class="n">math</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="nb">input</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span> <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">math</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="nb">input</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span> <span class="o">/</span> <span class="p">(</span> <span class="mf">1.</span> <span class="o">+</span> <span class="n">math</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="nb">input</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span> <span class="p">)</span>

    <span class="n">out</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">res</span>

<span class="o">@</span><span class="n">cuda</span><span class="p">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">sigmoid_backward</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">input_len</span><span class="p">,</span> <span class="n">out</span><span class="p">):</span>
    <span class="n">cbi</span><span class="p">,</span><span class="n">cbd</span><span class="p">,</span><span class="n">tid</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="n">blockIdx</span><span class="p">,</span><span class="n">cuda</span><span class="p">.</span><span class="n">blockDim</span><span class="p">,</span><span class="n">cuda</span><span class="p">.</span><span class="n">threadIdx</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">cbi</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">cbd</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">tid</span><span class="p">.</span><span class="n">x</span>

    <span class="k">if</span> <span class="n">idx</span> <span class="o">&gt;=</span> <span class="n">input_len</span><span class="p">:</span>
        <span class="k">return</span>
    
    <span class="n">out</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="nb">input</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
</code></pre></div></div>
<p>There is a lot to unpack here, so let’s start with the first lines. We are accessing <strong><em>cuda.blockIdx</em></strong> and <strong><em>cuda.threadIdx</em></strong> to get our block and thread indexes, and <strong><em>cuda.blockDim</em></strong> to know how many threads we have per block. And since we are using a single thread to compute a single value from our input tensor, we get our final index with</p>

<p>\(idx = B_{index} * B_{size} + T_{index}\),</p>

<p>where \(B_{size}\) is the number of threads per block, \(B_{index}\) and \(T_{index}\) are the block and thread indexes.</p>

<p>Having our current index, we must check if this index is within our input tensor size, and return without doing anything if it is not. Those cases will happen when the total number of threads, i.e. number of thread blocks times block size, is not exactly the same as the input size.</p>

<p>If everything is correct, we will take the current value from input at location \(idx\) and calculate our Sigmoid. Nothing too fancy here. But we can test with the following code:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">sigmoid_numba</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">fun</span><span class="p">,</span> <span class="n">tw</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">gradcheck</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="p">(</span><span class="n">input_len</span><span class="p">,)</span> <span class="o">=</span> <span class="nb">input</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">input_len</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="p">.</span><span class="n">contiguous</span><span class="p">().</span><span class="n">cuda</span><span class="p">()</span>
    <span class="n">tpb</span> <span class="o">=</span> <span class="n">tw</span>
    <span class="n">blocks</span> <span class="o">=</span> <span class="n">cdiv</span><span class="p">(</span><span class="n">input_len</span><span class="p">,</span><span class="n">tpb</span><span class="p">)</span>
    <span class="n">fun</span><span class="p">[</span><span class="n">blocks</span><span class="p">,</span> <span class="n">tpb</span><span class="p">](</span><span class="nb">input</span><span class="p">,</span> <span class="n">input_len</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span> 
    <span class="k">return</span> <span class="n">out</span>
    
<span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">as_tensor</span><span class="p">([</span><span class="mf">0.3</span><span class="p">,</span> <span class="o">-</span><span class="mi">100000</span><span class="p">,</span> <span class="mi">100000</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="p">.</span><span class="n">contiguous</span><span class="p">().</span><span class="n">cuda</span><span class="p">()</span>

<span class="n">res</span> <span class="o">=</span> <span class="n">sigmoid_numba</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">sigmoid_forward</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">grad</span> <span class="o">=</span> <span class="n">sigmoid_numba</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">sigmoid_backward</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>I’ve created an auxiliary function called <strong><em>sigmoid_numba</em></strong> to encapsulate the important (and boring) code necessary to allocate our output tensor and calculate an appropriate number of threads per block and thread blocks. Those configurations have some upper limits depending on your CUDA GPU, and the optimal value for each also depends on the GPU version. But for now, we are just going with some numbers that somewhat seem right, and in the end, we can run a small benchmark to decide the best values for our particular GPU. And finally, notice that our input tensor is calling two functions: contiguous() and cuda(): <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.contiguous.html#torch.Tensor.contiguous">contiguous</a> makes sure that our tensor is contiguous in memory since we are accessing it like a single dimensional array; <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cuda.html#torch.Tensor.cuda">cuda</a> returns a copy of our tensor in CUDA memory.</p>

<p>And that’s it, with this code you are programming a CUDA kernel, but with the big difference that we can use a debugger and step to our code as we wish, and with a much smaller iteration time :). Notice that it is best to set \(B_{size}=1\) when doing breakpoints since the debuggers usually don’t work well with multiple threads calling a breakpoint at the same time.</p>

<p>This Numba CUDA development is way easier, and if we change our env variable to NUMBA_ENABLE_CUDASIM=’0’ we can run this code that Numba will compile it to CUDA for us, and we can see the performance that we should get. For some reason, the direct implementation in C CUDA is usually faster, with differences of 2x to be expected, but even then it should show us how fast our final implementation should be. Notice, however, that without CUDA Simulator enabled we will losse the ability to debug our code and use numpy/torch functions. You can check out <a href="https://numba.pydata.org/numba-doc/latest/cuda/cudapysupported.html">here</a> what is supported.</p>

<h2 id="4-calling-out-chat-gpt-to-help-us">4. Calling out chat-GPT to Help Us</h2>

<p>The Numba development is there to help us, but the final goal is to generate a C CUDA kernel that we can directly call on PyTorch. Fortunately, Chat-GPT is plenty capable of doing this! I’ve pasted the following query, followed by the Numba code: “Convert the following python code to C CUDA kernel. Also add a function that uses torch library to pass the input arguments, call the CUDA kernel, check for errors. The function must receive torch::Tensor as input and return the output as torch::Tensor.”</p>

<p>And it gave me the something really close to this:</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;math.h&gt;</span><span class="cp">
</span>
<span class="n">__global__</span> <span class="kt">void</span> <span class="nf">sigmoid_forward_cuda_kernel</span><span class="p">(</span><span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">input</span><span class="p">,</span> <span class="kt">int</span> <span class="n">input_len</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">out</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">idx</span> <span class="o">&lt;</span> <span class="n">input_len</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">float</span> <span class="n">res</span><span class="p">;</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">input</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">res</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">+</span> <span class="n">expf</span><span class="p">(</span><span class="o">-</span><span class="n">input</span><span class="p">[</span><span class="n">idx</span><span class="p">]));</span>
        <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
            <span class="n">res</span> <span class="o">=</span> <span class="n">expf</span><span class="p">(</span><span class="n">input</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">+</span> <span class="n">expf</span><span class="p">(</span><span class="n">input</span><span class="p">[</span><span class="n">idx</span><span class="p">]));</span>
        <span class="p">}</span>

        <span class="n">out</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">res</span><span class="p">;</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">sigmoid_forward_cuda</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">input</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">CHECK_INPUT</span><span class="p">(</span><span class="n">input</span><span class="p">);</span>
    <span class="c1">// Get the data pointers and sizes</span>
    <span class="kt">float</span><span class="o">*</span> <span class="n">input_data_ptr</span> <span class="o">=</span> <span class="n">input</span><span class="p">.</span><span class="n">data_ptr</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">();</span>
    <span class="kt">int</span> <span class="n">input_len</span> <span class="o">=</span> <span class="n">input</span><span class="p">.</span><span class="n">numel</span><span class="p">();</span>

    <span class="c1">// Allocate output tensor on GPU</span>
    <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">out_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">::</span><span class="n">empty_like</span><span class="p">(</span><span class="n">input</span><span class="p">);</span>

    <span class="c1">// Get the data pointer for the output tensor</span>
    <span class="kt">float</span><span class="o">*</span> <span class="n">out_data_ptr</span> <span class="o">=</span> <span class="n">out_tensor</span><span class="p">.</span><span class="n">data_ptr</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">();</span>

    <span class="c1">// Set block and grid dimensions</span>
    <span class="kt">int</span> <span class="n">threads_per_block</span> <span class="o">=</span> <span class="mi">256</span><span class="p">;</span> <span class="c1">// You may adjust this based on your specific GPU capabilities</span>
    <span class="kt">int</span> <span class="n">num_blocks</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_len</span> <span class="o">+</span> <span class="n">threads_per_block</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">threads_per_block</span><span class="p">;</span>

    <span class="c1">// Launch CUDA kernel</span>
    <span class="n">sigmoid_forward_cuda_kernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">num_blocks</span><span class="p">,</span> <span class="n">threads_per_block</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">input_data_ptr</span><span class="p">,</span> <span class="n">input_len</span><span class="p">,</span> <span class="n">out_data_ptr</span><span class="p">);</span>

    <span class="c1">// Synchronize to ensure the kernel is done before proceeding</span>
    <span class="n">cudaDeviceSynchronize</span><span class="p">();</span>
    <span class="n">C10_CUDA_KERNEL_LAUNCH_CHECK</span><span class="p">();</span>

    <span class="k">return</span> <span class="n">out_tensor</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Notice that in the query we’ve explicitly told chat-GPT to accept a torch::tensor as input and return another one as output. This makes our lives so much easier in the following steps.</p>

<p>The backward pass is quite similar, and you can check it on my <a href="https://github.com/gfickel/cuda-sigmoid">repo</a>.</p>

<h2 id="5-using-pytorch-to-compile-c-cuda">5. Using PyTorch to Compile C CUDA</h2>

<p>I really didn’t know that PyTorch could do this, but if you have the dev files for CUDA and ninja build installed on your system you can pass the C CUDA code as a string and it will build it as a Python module for you. So first, to set things up we must have some auxiliary functions (thanks to Jeremy Howard), that you can check out <a href="https://github.com/gfickel/cuda-sigmoid/blob/main/utils.py">here</a>. The most important bit is a helper function to call <a href="https://pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.load_inline">load_inline</a> from PyTorch. It enables us to pass a C CUDA code as a string and compile it to a Python model containing the kernel as a Python function. It is quite amazing.</p>

<p>So, let’s compile our C CUDA kernel! Here are the steps:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cuda_src</span> <span class="o">=</span> <span class="n">FORWARD_PASS_CUDA_CODE_FROM_CHAT_GPT</span>
<span class="n">fname</span> <span class="o">=</span> <span class="s">'sigmoid_forward_cuda'</span>
<span class="n">cpp_src</span> <span class="o">=</span> <span class="s">'torch::Tensor sigmoid_forward_cuda(torch::Tensor input);'</span>

<span class="n">module_forward</span> <span class="o">=</span> <span class="n">load_cuda</span><span class="p">(</span><span class="n">cuda_src</span><span class="p">,</span> <span class="n">cpp_src</span><span class="p">,</span> <span class="p">[</span><span class="n">fname</span><span class="p">])</span>

<span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">as_tensor</span><span class="p">([</span><span class="mf">0.3</span><span class="p">,</span> <span class="o">-</span><span class="mi">100000</span><span class="p">,</span> <span class="mi">100000</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="p">.</span><span class="n">contiguous</span><span class="p">().</span><span class="n">cuda</span><span class="p">()</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">module_forward</span><span class="p">.</span><span class="n">sigmoid_forward_cuda</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre></div></div>
<p>And that’s it! But first, let’s explain those lines a little bit. First, <strong><em>cuda_src</em></strong> is a Python string containing our code that was so gently translated to us by chat GPT. <strong><em>fname</em></strong> is the function name that we want to expose as a function in our compiled module, and <strong><em>cpp_src</em></strong> is the C++ code that is compiled with our CUDA kernel, and all it has is the declaration of our function. With all of this, we can finally call our helper <strong><em>load_cuda</em></strong>, defined in our <strong><em>utils.py</em></strong> if you want to check it out, and it returns our new Python module with our <strong><em>sigmoid_forward_cuda</em></strong> function.</p>

<p>For the backward pass, it is mostly the same process, as expected. Here it is:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cuda_src</span> <span class="o">=</span> <span class="n">BACKWARD_PASS_CUDA_CODE_FROM_CHAT_GPT</span>
<span class="n">fname</span> <span class="o">=</span> <span class="s">'sigmoid_backward_cuda'</span>
<span class="n">cpp_src</span> <span class="o">=</span> <span class="s">'torch::Tensor sigmoid_backward_cuda(torch::Tensor input);'</span>

<span class="n">module_backward</span> <span class="o">=</span> <span class="n">load_cuda</span><span class="p">(</span><span class="n">cuda_src</span><span class="p">,</span> <span class="n">cpp_src</span><span class="p">,</span> <span class="p">[</span><span class="n">fname</span><span class="p">])</span>

<span class="n">grad</span> <span class="o">=</span> <span class="n">module_backward</span><span class="p">.</span><span class="n">sigmoid_backward_cuda</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="6-check-our-gradients-again">6. Check our Gradients Again</h2>

<p>Great, we have both our forward and backward passes implemented in CUDA! However, are they correct? Did chat gpt make some silly mistake on the translation part? Well, at least we can check if the backward is indeed the correct derivation for the forward pass. Just like we did in step 2, we must call checkgradients. And to do this, first, we must adhere to the autograd interface, like this:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">CUDASigmoid</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="n">Function</span><span class="p">):</span>
    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">module_forward</span><span class="p">.</span><span class="n">sigmoid_forward_cuda</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="n">ctx</span><span class="p">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">result</span>

    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="p">(</span><span class="n">result</span><span class="p">,)</span> <span class="o">=</span> <span class="n">ctx</span><span class="p">.</span><span class="n">saved_tensors</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">module_forward</span><span class="p">.</span><span class="n">sigmoid_backward_cuda</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">grad_output</span> <span class="o">*</span> <span class="n">grad</span>
</code></pre></div></div>

<p>Not that bad, if you ask me, and not that different from the one from step 2. And now, for the finale:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sigmoid</span> <span class="o">=</span> <span class="n">CUDASigmoid</span><span class="p">.</span><span class="nb">apply</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># Changing eps and atol since we are dealing with float32
</span><span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="n">gradcheck</span><span class="p">(</span><span class="n">sigmoid</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'gradcheck successful :D'</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'gradcheck unsuccessful :D'</span><span class="p">)</span>
</code></pre></div></div>
<p>Wait, something is different. Our <strong><em>eps</em></strong> and <strong><em>atol</em></strong> are smaller, and our test data is float32 instead of float64. This final difference is indeed the key: gradcheck is made to work with float64, otherwise we will have some larger errors from our floating point operations. And since we’ve only implemented in float32 we must lower our error thresholds. It was indeed quite possible to use C++ templates and generate a double (i.e. float64) kernel also, but it was going to introduce some unnecessary complications for now.</p>

<p>With those caveats aside, our gradcheck should be passing and we are officially golden, our CUDA Sigmoid implementation is over!</p>

<h2 id="conclusions">Conclusions</h2>

<p>Uou, that was a long post. However, I tried to skim only the not-critical details and explain in greater detail the development pipeline. That is the key point that you should be taking from here: how to make CUDA development less sucky. And by using this PyTorch feature to compile CUDA code, we can even run CUDA kernels on Google Collabs!</p>

<p>In the end, I do believe that this is an interesting knowledge to have, and in this day and age of huge LLMs, being able to tackle some performance bottlenecks can have a great impact.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[I still remember the “dark ages” of research, when I was still doing my masters when it was common to find really impactful publications that provided no code. And yes, I’ve sent my fair share of emails to authors… Fortunately, this is no longer the norm, and even somewhat frowned upon. Caffe, Tensorflow, Keras, PyTorch, and even more deep learning frameworks really helped everyone to create way smaller, cleaner code, that was also easier to share.]]></summary></entry><entry><title type="html">Welcome to Jekyll!</title><link href="http://localhost:4000/jekyll/update/2024/02/22/welcome-to-jekyll.html" rel="alternate" type="text/html" title="Welcome to Jekyll!" /><published>2024-02-22T19:55:11-03:00</published><updated>2024-02-22T19:55:11-03:00</updated><id>http://localhost:4000/jekyll/update/2024/02/22/welcome-to-jekyll</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/02/22/welcome-to-jekyll.html"><![CDATA[<p>You’ll find this post in your <code class="language-plaintext highlighter-rouge">_posts</code> directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run <code class="language-plaintext highlighter-rouge">jekyll serve</code>, which launches a web server and auto-regenerates your site when a file is updated.</p>

<p>Jekyll requires blog post files to be named according to the following format:</p>

<p><code class="language-plaintext highlighter-rouge">YEAR-MONTH-DAY-title.MARKUP</code></p>

<p>Where <code class="language-plaintext highlighter-rouge">YEAR</code> is a four-digit number, <code class="language-plaintext highlighter-rouge">MONTH</code> and <code class="language-plaintext highlighter-rouge">DAY</code> are both two-digit numbers, and <code class="language-plaintext highlighter-rouge">MARKUP</code> is the file extension representing the format used in the file. After that, include the necessary front matter. Take a look at the source for this post to get an idea about how it works.</p>

<p>Jekyll also offers powerful support for code snippets:</p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="k">def</span> <span class="nf">print_hi</span><span class="p">(</span><span class="nb">name</span><span class="p">)</span>
  <span class="nb">puts</span> <span class="s2">"Hi, </span><span class="si">#{</span><span class="nb">name</span><span class="si">}</span><span class="s2">"</span>
<span class="k">end</span>
<span class="n">print_hi</span><span class="p">(</span><span class="s1">'Tom'</span><span class="p">)</span>
<span class="c1">#=&gt; prints 'Hi, Tom' to STDOUT.</span></code></pre></figure>

<p>Check out the <a href="https://jekyllrb.com/docs/home">Jekyll docs</a> for more info on how to get the most out of Jekyll. File all bugs/feature requests at <a href="https://github.com/jekyll/jekyll">Jekyll’s GitHub repo</a>. If you have questions, you can ask them on <a href="https://talk.jekyllrb.com/">Jekyll Talk</a>.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[You’ll find this post in your _posts directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run jekyll serve, which launches a web server and auto-regenerates your site when a file is updated.]]></summary></entry><entry><title type="html">100x speedup on Python with just a touch of C++</title><link href="http://localhost:4000/jekyll/update/2022/05/13/100x-speedup-on-python-with-just-a-touch-of-c.html" rel="alternate" type="text/html" title="100x speedup on Python with just a touch of C++" /><published>2022-05-13T12:00:00-03:00</published><updated>2022-05-13T12:00:00-03:00</updated><id>http://localhost:4000/jekyll/update/2022/05/13/100x-speedup-on-python-with-just-a-touch-of-c</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2022/05/13/100x-speedup-on-python-with-just-a-touch-of-c.html"><![CDATA[<p>Python is a great language. I still remember my first contact with Python2 some 8 years ago, and I was amazed by how clean and expressive it was. And now, with Python3, a lot has changed. It is now the de facto language for machine learning (so long, Matlab!), and lots of amazing stuff have been built with it.</p>

<p>All is good and dandy, however from time to time I’ve encountered a brick wall when working on Python: how slow it is. Don’t get me wrong, if you are using libs to do your heavy processing, such as NumPy, you are good to go. But it’s important to notice that the core of NumPy is not Python, and for a reason. It’s just not the language for that.</p>

<p>For most cases, you can use such libs and pass those crunch-intensive stuff to them, but sometimes you want something not so conventional and that does not conform with such limitations. And then you end up writing two nested fors in Python, processing a Full HD image, and you want to cry…</p>

<p>Fortunately, we can write those code hot spots in C++, and it is surprisingly simple to do it and seamlessly integrate with Python. However, this opens another can of worms that is C++, and its dependencies and compatibilities. For anyone that had to target Linux, Windows in both 32 and 64 bits should know what I’m talking about. So for me it is of the utmost importance that it can be used seamlessly in any platform without any dependencies other than a C++ compiler.</p>

<p>So upfront I’m already discarding <a href="https://www.boost.org/doc/libs/1_66_0/libs/python/doc/html/index.html">Boost.Python</a> and <a href="https://github.com/pybind/pybind11">PyBind11</a>. I’ve used both, and usually prefer PyBind11 since it is much easier to manage on different platforms. But one dependency is one too many. And as I will show it now, you don’t need them for most cases.</p>

<p>Let’s start with a very simple and naive example: normalize the contrast of a black and white image.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">naive_contrast_image</span><span class="p">(</span><span class="n">image</span><span class="p">):</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">image</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">uint8</span><span class="p">)</span>
    <span class="n">min_color</span><span class="p">,</span> <span class="n">max_color</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">min</span><span class="p">(</span><span class="n">image</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
    <span class="n">delta_color</span> <span class="o">=</span> <span class="n">max_color</span><span class="o">-</span><span class="n">min_color</span>
    <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">image</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">image</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
            <span class="n">pixel</span> <span class="o">=</span> <span class="n">image</span><span class="p">[</span><span class="n">row</span><span class="p">,</span><span class="n">col</span><span class="p">]</span>
            <span class="n">result</span><span class="p">[</span><span class="n">row</span><span class="p">,</span><span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="mi">255</span><span class="o">*</span><span class="p">(</span><span class="n">pixel</span><span class="o">-</span><span class="n">min_color</span><span class="p">)</span><span class="o">/</span><span class="n">delta_color</span>

    <span class="k">return</span> <span class="n">result</span>
</code></pre></div></div>

<p>So this code generates the following result:</p>

<p><img src="/assets/turing.png" alt="Contrast example image" /></p>

<p>This is a very simple and naive example that could (and should) be done using NumPy. But let us do this in C++.</p>

<p>The first difference in C++ is that you should specify the variable types. So let us define image as an np.uint8 array, and the resulting image with the same type. On C++ this can be represented as unsigned char. Let’s take a look at our implementation. On contrast_image.h:</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;algorithm&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;vector&gt;</span><span class="cp">
</span>
<span class="k">extern</span> <span class="s">"C"</span> <span class="p">{</span>

<span class="kt">void</span> <span class="n">cpp_contrast_image</span><span class="p">(</span><span class="k">const</span> <span class="kt">unsigned</span> <span class="kt">char</span> <span class="o">*</span><span class="n">image</span><span class="p">,</span> <span class="kt">int</span> <span class="n">height</span><span class="p">,</span> <span class="kt">int</span> <span class="n">width</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">char</span> <span class="o">*</span><span class="n">outResult</span><span class="p">);</span>

<span class="p">}</span> <span class="c1">// extern "C"</span>
</code></pre></div></div>

<p>And contrast_image.cpp:</p>
<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="cp">#include</span> <span class="cpf">"contrast_image.h"</span><span class="cp">
</span>
<span class="kt">void</span> <span class="nf">cpp_contrast_image</span><span class="p">(</span><span class="k">const</span> <span class="kt">unsigned</span> <span class="kt">char</span> <span class="o">*</span><span class="n">image</span><span class="p">,</span> <span class="kt">int</span> <span class="n">height</span><span class="p">,</span> <span class="kt">int</span> <span class="n">width</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">char</span> <span class="o">*</span><span class="n">outResult</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">auto</span> <span class="n">vec</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">unsigned</span> <span class="kt">char</span><span class="o">&gt;</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">image</span><span class="o">+</span><span class="n">width</span><span class="o">*</span><span class="n">height</span><span class="p">);</span>
    <span class="k">auto</span> <span class="n">minmax</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">minmax_element</span><span class="p">(</span><span class="n">vec</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">vec</span><span class="p">.</span><span class="n">end</span><span class="p">());</span>
    <span class="kt">float</span> <span class="n">min</span> <span class="o">=</span> <span class="p">(</span><span class="kt">float</span><span class="p">)</span><span class="o">*</span><span class="n">minmax</span><span class="p">.</span><span class="n">first</span><span class="p">;</span>
    <span class="kt">float</span> <span class="n">max</span> <span class="o">=</span> <span class="p">(</span><span class="kt">float</span><span class="p">)</span><span class="o">*</span><span class="n">minmax</span><span class="p">.</span><span class="n">second</span><span class="p">;</span>
    <span class="kt">float</span> <span class="n">delta_color</span> <span class="o">=</span> <span class="n">max</span><span class="o">-</span><span class="n">min</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">row</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">row</span><span class="o">&lt;</span><span class="n">height</span><span class="p">;</span> <span class="n">row</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">col</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">col</span><span class="o">&lt;</span><span class="n">width</span><span class="p">;</span> <span class="n">col</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="kt">int</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">row</span><span class="o">*</span><span class="n">width</span> <span class="o">+</span> <span class="n">col</span><span class="p">;</span>
            <span class="kt">float</span> <span class="n">pixel</span> <span class="o">=</span> <span class="p">(</span><span class="kt">float</span><span class="p">)</span><span class="n">image</span><span class="p">[</span><span class="n">idx</span><span class="p">];</span>
            <span class="n">outResult</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="kt">char</span><span class="p">)(</span><span class="mi">255</span><span class="o">*</span><span class="p">(</span><span class="n">pixel</span><span class="o">-</span><span class="n">min</span><span class="p">)</span><span class="o">/</span><span class="n">delta_color</span><span class="p">);</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>There are some small but very important details here, so let’s start with the important ones.</p>

<ol>
  <li>Avoid dynamic memory allocation on C++. Python Garbage Collector will not see them so you will have to free them by yourself. Prefer to allocate the memory with NumPy. This will be shown further along.</li>
  <li>Multiple dimensional arrays are actually just a single array with some syntactic sugar to access it. You’ll notice the direct idx calculation on the example. It is a good practice to create a function to give you the index given the desired position to avoid silly bugs.</li>
  <li>Access and/or modify an invalid array position will generate the dreadful Segmentation Fault. So always be diligent with the range checks.</li>
  <li>The function must have a C compatible interface, as we can see with the extern “C” on contrast_image.h. Usually this is not a big deal since we can use all the desired C++ stuff within the implementation on contrast_image.cpp, however we will have to implement different versions for different input types since templates are not available on the function definition :(.</li>
</ol>

<p>Finally, returning complex objects within a C interface is not the easiest and cleanest thing to do. So for the most part I just reserve my final arguments to return my value. And also, use const on every array that you should not change and let the compiler help you find bugs.</p>

<p>Ok, we have a C++ code that does exactly what we want and can compile it to a lib with:</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>g++ <span class="nt">-Wall</span> <span class="nt">-O2</span> <span class="nt">-c</span> <span class="nt">-fPIC</span> contrast_image.cpp

g++ contrast_image.o <span class="nt">-shared</span> <span class="nt">-o</span> libcontrast_image.so
</code></pre></div></div>

<p>Until now I did not say anything out of the ordinary, but we are surprisingly close to finishing it. Python has a useful and easy way to access a C compiled libs using ctypes. So this is how we will use our cpp_contrast_image on Python:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">ctypes</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">numpy.ctypeslib</span> <span class="kn">import</span> <span class="n">ndpointer</span>

<span class="n">lib</span> <span class="o">=</span> <span class="n">ctypes</span><span class="p">.</span><span class="n">CDLL</span><span class="p">(</span><span class="s">'./libcontrast_image.so'</span><span class="p">)</span>

<span class="n">c_contrast_image</span> <span class="o">=</span> <span class="n">lib</span><span class="p">.</span><span class="n">cpp_contrast_image</span>
<span class="n">c_contrast_image</span><span class="p">.</span><span class="n">argtypes</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">ndpointer</span><span class="p">(</span><span class="n">ctypes</span><span class="p">.</span><span class="n">c_ubyte</span><span class="p">,</span> <span class="n">flags</span><span class="o">=</span><span class="s">'C_CONTIGUOUS'</span><span class="p">),</span>
    <span class="n">ctypes</span><span class="p">.</span><span class="n">c_int</span><span class="p">,</span>
    <span class="n">ctypes</span><span class="p">.</span><span class="n">c_int</span><span class="p">,</span>
    <span class="n">ndpointer</span><span class="p">(</span><span class="n">ctypes</span><span class="p">.</span><span class="n">c_ubyte</span><span class="p">,</span> <span class="n">flags</span><span class="o">=</span><span class="s">'C_CONTIGUOUS'</span><span class="p">),</span>
<span class="p">]</span>

<span class="k">def</span> <span class="nf">contrast_image</span><span class="p">(</span><span class="n">image</span><span class="p">):</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">image</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">uint8</span><span class="p">)</span>
    <span class="n">c_contrast_image</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">image</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">image</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">result</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span>
</code></pre></div></div>

<p>And that’s it! You can use the new contrast_image python function with exactly the same interface, but much faster! How fast, you may ask. Well, on my i7 8550-U it went from 1229.050ms to 1.645ms on this demo image. Quite a difference! That’s actually over 700x faster, way over the promised 100x. The reason is that in our use cases we often see a speedup of a little over 100 times, so I’m trying to not over-promise here.</p>

<p>Just as with our C++ code, we have some important stuff to notice here. So let’s do it:</p>

<ol>
  <li>On C++ we treated our NumPy arrays as a single contiguous array. Usually that is the case, but not always! Fortunately we can explicit this constraint on Python itself, informing that our NumPy array is of type char and must be contiguous. If you call it with the wrong type an exception will be raised, saving you from a possible Segmentation Fault. You can check the available c_types <a href="https://docs.python.org/3/library/ctypes.html">here</a>.</li>
  <li>Remember that we are avoiding to allocate memory on the C++ code? So we are doing it here, by explicitly allocating the result image with np.zeros.</li>
  <li>We have to explicitly point to where our compiled C++ library is to be loaded from, using ctypes.CDLL.</li>
</ol>

<p>That’s it! Within a few lines of code you have lots of freedom to easily integrate C++ code into Python, and all of that without any dependency :)</p>

<p>You may be thinking that this is a silly example. And you are right. But you can do lots of stuff with this knowledge. For example, we decreased the runtime of a rasterization algorithm from 2.5s to 1.8ms, quite a hefty difference! You can read all of that on a following post to be released. But I’ll warn you, it was really easy :)</p>

<p>Finally, I must quote a great thinker: “With great powers comes great responsibility”. For an untrained person dabbling with pointers at C++ is a quick road to memory leaks and Segmentation Faults. Actually, even for trained ones. So it is a good practice to keep those codes as short as possible, usually not replacing a whole function but just the slow parts. And don’t forget to do lots of unit tests to catch some unusual edge cases. But if you are willing to deal with those drawbacks, a whole new world of crazy fast code awaits you!</p>

<p>PS.: All of this code and the benchmark script can be seen on <a href="https://github.com/gfickel/python_cpp">https://github.com/gfickel/python_cpp</a>. It is meant to only illustrate the interface between C++ and Python, so everything surrounding it is not production ready. This is up to the reader ;)</p>

<p>PS2.: Thanks to Michele Tanus, Gustavo Führ and Roger Granada for proofreading and greatly improving this post.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Python is a great language. I still remember my first contact with Python2 some 8 years ago, and I was amazed by how clean and expressive it was. And now, with Python3, a lot has changed. It is now the de facto language for machine learning (so long, Matlab!), and lots of amazing stuff have been built with it.]]></summary></entry></feed>