<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Making your GPU go BRRR - Programing a PyTorch Layer in CUDA | GFickel Blog</title>
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="Making your GPU go BRRR - Programing a PyTorch Layer in CUDA" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="I still remember the “dark ages” of research, when I was still doing my masters, that it was common to find really impactfull publications that provided no code. And yes, I’ve send my fair share of emails to authors… Fortunatelly, this is no longer the norm, and even somewhat frowned upon. Caffe, Tensorflow, Keras, PyTorch, and even more deeplearning frameworks really helped everyone to create way smaller, cleaner code, that were also easier to share." />
<meta property="og:description" content="I still remember the “dark ages” of research, when I was still doing my masters, that it was common to find really impactfull publications that provided no code. And yes, I’ve send my fair share of emails to authors… Fortunatelly, this is no longer the norm, and even somewhat frowned upon. Caffe, Tensorflow, Keras, PyTorch, and even more deeplearning frameworks really helped everyone to create way smaller, cleaner code, that were also easier to share." />
<link rel="canonical" href="http://localhost:4000/jekyll/update/2024/02/23/making-your-gpu-go-brrr-programing-a-network-layer-in-cuda.html" />
<meta property="og:url" content="http://localhost:4000/jekyll/update/2024/02/23/making-your-gpu-go-brrr-programing-a-network-layer-in-cuda.html" />
<meta property="og:site_name" content="GFickel Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-02-23T12:00:00-03:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Making your GPU go BRRR - Programing a PyTorch Layer in CUDA" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-02-23T12:00:00-03:00","datePublished":"2024-02-23T12:00:00-03:00","description":"I still remember the “dark ages” of research, when I was still doing my masters, that it was common to find really impactfull publications that provided no code. And yes, I’ve send my fair share of emails to authors… Fortunatelly, this is no longer the norm, and even somewhat frowned upon. Caffe, Tensorflow, Keras, PyTorch, and even more deeplearning frameworks really helped everyone to create way smaller, cleaner code, that were also easier to share.","headline":"Making your GPU go BRRR - Programing a PyTorch Layer in CUDA","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/jekyll/update/2024/02/23/making-your-gpu-go-brrr-programing-a-network-layer-in-cuda.html"},"url":"http://localhost:4000/jekyll/update/2024/02/23/making-your-gpu-go-brrr-programing-a-network-layer-in-cuda.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="GFickel Blog" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">GFickel Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Making your GPU go BRRR - Programing a PyTorch Layer in CUDA</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2024-02-23T12:00:00-03:00" itemprop="datePublished">Feb 23, 2024
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>I still remember the “dark ages” of research, when I was still doing my masters, that it was common to find really impactfull publications that provided no code. And yes, I’ve send my fair share of emails to authors… Fortunatelly, this is no longer the norm, and even somewhat frowned upon. Caffe, Tensorflow, Keras, PyTorch, and even more deeplearning frameworks really helped everyone to create way smaller, cleaner code, that were also easier to share.</p>

<p>Those frameworks are really incredible, and allow us to quickly implement and test new ideas, however they are not always the fastest way, even if they using CUDA down the line. A good example is the amazing work of Flash Attention, that implemented the Attention machanism (yeah, the one used on all LLMs right now) in CUDA, and achieved a great runtime improvement.</p>

<p>CUDA programming may seem intimidating, at least it was for me. I first learned circa 2010 and it was a really bad development experience, but by watching an <a href="https://www.youtube.com/watch?v=nOxKexn3iBo">awesome video</a> by Jeremy Howard, I’ve learned that it is indeed possible to have a much better experience. And the main idea is the following:</p>

<ol>
  <li>Implement the forward and backward pass in PyTorch. This gives access to an online debugger and the full functionality of Python, like Jupyter Notebooks.</li>
  <li>Validate the implementation with gradcheck. This somewhat magic function, runs your forward pass and do numerical derivation to validate your backward pass code.</li>
  <li>Program the CUDA Kernel for forward and backward pass using Numba, directly in Python. This is the real thing, where we are dealing CUDA threads and possibly memory management.</li>
  <li>Ask <a href="https://chat.openai.com/">Chat-GPT</a> to convert this code to C CUDA. Really, it works surprisingly well!</li>
  <li>Use PyTorch internal functionality to compile this C CUDA to a Python module that you can use with torch tensors.</li>
  <li>Use gradcheck again to verify that your CUDA written layer is 100% correct.</li>
</ol>

<p>It may be a couple of hoops, but the ability to develop CUDA code in Python makes our lives so much easier. You have easier integration with debugers, and the iteration time between changes in code and running it is nearly instant, compared to the long time it takes to compile C CUDA. And you may noticed that mentioned both forward and backward pass, and unfortunatly, if we use CUDA for our backward pass, we can’t rely on autograd to get this for us. But fortunatly we have this amazing funtion from PyTorch, <a href="https://pytorch.org/docs/stable/notes/gradcheck.html">gradcheck</a>, that will validate for us if our backpropagation is indeed correct.</p>

<p>So let’s stop talking and lets start coding. For our example I’ve chosen the Sigmoid activation that have some interesting characteristics.</p>

<h2 id="implement-in-pytorch">Implement in PyTorch</h2>

<p>The idea is</p>

\[x = {-b \pm \sqrt{b^2-4ac} \over 2a}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">code</span>
</code></pre></div></div>

  </div><a class="u-url" href="/jekyll/update/2024/02/23/making-your-gpu-go-brrr-programing-a-network-layer-in-cuda.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">GFickel Blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">GFickel Blog</li><li><a class="u-email" href="mailto:guilhermefickel@gmail.com">guilhermefickel@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/gfickel"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">gfickel</span></a></li><li><a href="https://www.twitter.com/guilhermefickel"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">guilhermefickel</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>A Blog mostly geared toward Machine Learning and high performance code.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
